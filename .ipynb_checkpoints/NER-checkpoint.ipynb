{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8208ccc7-447f-4d43-80f2-5796c1eea7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.15.2\n",
      "  Downloading torchtext-0.15.2-cp39-cp39-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext==0.15.2) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext==0.15.2) (2.32.3)\n",
      "Collecting torch==2.0.1 (from torchtext==0.15.2)\n",
      "  Using cached torch-2.0.1-cp39-cp39-win_amd64.whl.metadata (23 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext==0.15.2) (1.24.1)\n",
      "Collecting torchdata==0.6.1 (from torchtext==0.15.2)\n",
      "  Downloading torchdata-0.6.1-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.0.1->torchtext==0.15.2) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.0.1->torchtext==0.15.2) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.0.1->torchtext==0.15.2) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.0.1->torchtext==0.15.2) (3.0rc1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.0.1->torchtext==0.15.2) (3.1.2)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchdata==0.6.1->torchtext==0.15.2) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext==0.15.2) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext==0.15.2) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext==0.15.2) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->torchtext==0.15.2) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch==2.0.1->torchtext==0.15.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\andrey\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch==2.0.1->torchtext==0.15.2) (1.2.1)\n",
      "Downloading torchtext-0.15.2-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.9 MB 2.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.3/1.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.5/1.9 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 9.5 MB/s eta 0:00:00\n",
      "Using cached torch-2.0.1-cp39-cp39-win_amd64.whl (172.4 MB)\n",
      "Downloading torchdata-0.6.1-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 85.8 MB/s eta 0:00:00\n",
      "Installing collected packages: torch, torchdata, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1+cu117\n",
      "    Uninstalling torch-1.13.1+cu117:\n",
      "      Successfully uninstalled torch-1.13.1+cu117\n",
      "  Attempting uninstall: torchdata\n",
      "    Found existing installation: torchdata 0.7.1\n",
      "    Uninstalling torchdata-0.7.1:\n",
      "      Successfully uninstalled torchdata-0.7.1\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.10.0\n",
      "    Uninstalling torchtext-0.10.0:\n",
      "      Successfully uninstalled torchtext-0.10.0\n",
      "Successfully installed torch-2.0.1 torchdata-0.6.1 torchtext-0.15.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Andrey\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\~-rch'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 0.13.1+cu117 requires torch==1.13.1, but you have torch 2.0.1 which is incompatible.\n",
      "torchvision 0.14.1+cu117 requires torch==1.13.1, but you have torch 2.0.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torchtext==0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced5dc01-3d69-494b-a49b-f648f6d7c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrey\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\Andrey\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\Andrey\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "from typing import List\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GroupKFold, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218d9852-3f1d-4509-a74c-5d5515ebe533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_mountains = pd.read_csv('D:/datasets/mountain_dataset_with_markup.csv', converters={'marker': eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43881d99-ff0d-4426-95a1-aefa302f59dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>marker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A visit to a science museum for hands-on learn...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voice surface coach set democratic time year. ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Parent according maybe activity activity finis...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A visit to a sculpture garden with intriguing ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Julian Alps in Slovenia offer pristine lak...</td>\n",
       "      <td>[(11, 15)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>They never audience meet. Appear region allow ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>Witnessing the mesmerizing Northern Lights dan...</td>\n",
       "      <td>[(75, 97)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>Consumer join stage. Best likely center they p...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>Hospital real school cover hotel over. Any tra...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>A brilliant pass from the midfielder sets up a...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1584 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text      marker\n",
       "0     A visit to a science museum for hands-on learn...          []\n",
       "1     Voice surface coach set democratic time year. ...          []\n",
       "2     Parent according maybe activity activity finis...          []\n",
       "3     A visit to a sculpture garden with intriguing ...          []\n",
       "4     The Julian Alps in Slovenia offer pristine lak...  [(11, 15)]\n",
       "...                                                 ...         ...\n",
       "1579  They never audience meet. Appear region allow ...          []\n",
       "1580  Witnessing the mesmerizing Northern Lights dan...  [(75, 97)]\n",
       "1581  Consumer join stage. Best likely center they p...          []\n",
       "1582  Hospital real school cover hotel over. Any tra...          []\n",
       "1583  A brilliant pass from the midfielder sets up a...          []\n",
       "\n",
       "[1584 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mountains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a61e7964-b224-462b-8046-bb3ac091b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our synthetic data is pretty clean so no need in such brutal cleaning\n",
    "# Yet this is a great example of how to process data from different sources like \n",
    "# Telegram Twitter Instagram etc.\n",
    "def preprocess_text(text):\n",
    "    # Remove links\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "    # Special remove telegram links\n",
    "    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:t\\.me\\/\\S+|telegram\\.me\\/\\S+|telegram\\.dog\\/\\S+)\"\n",
    "    text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Remove phone numbers\n",
    "    phone_regex = r'\\(?\\+?\\d{0,3}\\)?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{2}[-.\\s]?\\d{2}'\n",
    "    text = re.sub(phone_regex, '', text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[\\n\\t\\r]', ' ', text)\n",
    "\n",
    "    # Remove tags\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        pattern=\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c43c6c1-3534-4e10-90b8-83d6e73617b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>marker</th>\n",
       "      <th>loc_markers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A visit to a science museum for hands-on learn...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voice surface coach set democratic time year. ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Parent according maybe activity activity finis...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A visit to a sculpture garden with intriguing ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Julian Alps in Slovenia offer pristine lak...</td>\n",
       "      <td>[(11, 15)]</td>\n",
       "      <td>[(11, 15)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>They never audience meet. Appear region allow ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>Witnessing the mesmerizing Northern Lights dan...</td>\n",
       "      <td>[(75, 97)]</td>\n",
       "      <td>[(75, 87), (88, 97)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>Consumer join stage. Best likely center they p...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>Hospital real school cover hotel over. Any tra...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>A brilliant pass from the midfielder sets up a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1584 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text      marker  \\\n",
       "0     A visit to a science museum for hands-on learn...          []   \n",
       "1     Voice surface coach set democratic time year. ...          []   \n",
       "2     Parent according maybe activity activity finis...          []   \n",
       "3     A visit to a sculpture garden with intriguing ...          []   \n",
       "4     The Julian Alps in Slovenia offer pristine lak...  [(11, 15)]   \n",
       "...                                                 ...         ...   \n",
       "1579  They never audience meet. Appear region allow ...          []   \n",
       "1580  Witnessing the mesmerizing Northern Lights dan...  [(75, 97)]   \n",
       "1581  Consumer join stage. Best likely center they p...          []   \n",
       "1582  Hospital real school cover hotel over. Any tra...          []   \n",
       "1583  A brilliant pass from the midfielder sets up a...          []   \n",
       "\n",
       "               loc_markers  \n",
       "0                       []  \n",
       "1                       []  \n",
       "2                       []  \n",
       "3                       []  \n",
       "4               [(11, 15)]  \n",
       "...                    ...  \n",
       "1579                    []  \n",
       "1580  [(75, 87), (88, 97)]  \n",
       "1581                    []  \n",
       "1582                    []  \n",
       "1583                    []  \n",
       "\n",
       "[1584 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def divide_markers(text, markers):\n",
    "    divided_markers = []\n",
    "    \n",
    "    for start, end in markers:\n",
    "        # Extract the mountain name from the text based on the marker\n",
    "        mountain_name = text[start:end]\n",
    "\n",
    "        # Split the mountain name into individual words\n",
    "        words = mountain_name.split(\" \")\n",
    "\n",
    "        # Generate divided markers for each word\n",
    "        for word in words:\n",
    "            word_start = text.find(word, start)\n",
    "            word_end = word_start + len(word)\n",
    "            divided_markers.append((word_start, word_end))\n",
    "\n",
    "    return divided_markers\n",
    "\n",
    "\n",
    "# Apply the divide_markers function to the DataFrame\n",
    "df_mountains['loc_markers'] = df_mountains.apply(lambda row: divide_markers(row['text'], row['marker']), axis=1)\n",
    "df_mountains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ddd254f-9180-40f2-90de-1b2134a56240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|████████████████████████████████████████████████████████████████████████████| 1584/1584 [00:00<00:00, 3790.41it/s]"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_mountains['loc_markers'] = df_mountains['loc_markers'].apply(lambda x: [[y[0], y[1], 'LOC'] for y in x])\n",
    "\n",
    "nlp = spacy.blank(\"xx\")\n",
    "\n",
    "def convert_to_conll(row):\n",
    "    data = {\n",
    "        \"text\": row['text'],\n",
    "        \"label\": row['loc_markers']\n",
    "    }\n",
    "    doc = nlp(data[\"text\"])\n",
    "    ents = []\n",
    "\n",
    "    # Sort the spans based on their start positions\n",
    "    sorted_spans = sorted(data[\"label\"], key=lambda x: x[0])\n",
    "\n",
    "    for start, end, label in sorted_spans:\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "\n",
    "        # Check for overlaps with existing spans\n",
    "        if span is not None:\n",
    "            if not any(span.start >= ent.start and span.end <= ent.end for ent in ents):\n",
    "                ents.append(span)\n",
    "        else:\n",
    "            pass\n",
    "            # TODO: fix not align to token case\n",
    "            # print(\"Skipping span (does not align to tokens):\", start, end, label, doc.text[start:end])\n",
    "\n",
    "    doc.ents = ents\n",
    "    return {\n",
    "        'tokens': list([t.text for t in doc]),\n",
    "        'labels': list(biluo_to_iob(doc_to_biluo_tags(doc)))\n",
    "    }\n",
    "\n",
    "df_mountains['conll'] = df_mountains.progress_apply(convert_to_conll, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce4a8fa-4f03-47fd-9361-f0775d16c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'O': 0, 'B-LOC': 1, 'I-LOC': 2}\n",
    "\n",
    "df_mountains['tokens'] = df_mountains.conll.str['tokens']\n",
    "df_mountains['ner_tags'] = df_mountains.conll.str['labels'].apply(lambda x: [label2id[t] for t in x])\n",
    "\n",
    "df_mountains['is_valid'] = 0\n",
    "df_mountains.loc[df_mountains.index > 1200, 'is_valid'] = 1\n",
    "\n",
    "df_train = df_mountains[df_mountains.is_valid == 0]\n",
    "df_valid = df_mountains[df_mountains.is_valid == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a27b2cf-477e-4b7f-b8ec-e48e70a2709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mountains[['tokens', 'ner_tags']].to_json(\n",
    "    'train_processed.json', orient='records', lines=True)\n",
    "df_mountains[['tokens', 'ner_tags']].to_json(\n",
    "    'valid_processed.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba440b1b-d6d6-43a2-86b8-23a2c42a567d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8ed560a599f655d4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/Andrey/.cache/huggingface/datasets/json/default-8ed560a599f655d4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                                                        ███████████████████████████████| 2/2 [00:00<00:00, 666.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/Andrey/.cache/huggingface/datasets/json/default-8ed560a599f655d4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 75.00it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 1584\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 1584\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        'train': 'train_processed.json',\n",
    "        'val': 'valid_processed.json'\n",
    "    }\n",
    ")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c075ed-58f3-45bc-8c7d-85a9b9591c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ownloading tokenizer.json: 100%|█████████████████████████████████████████████████| 9.10M/9.10M [00:00<00:00, 25.1MB/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'xlm-roberta-base',\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large'\n",
    "#                                           , add_prefix_space=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27a43964-d3d9-42f8-8a74-1b09f919901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd86cba7-da60-4c0d-a790-697dd42a62e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "00%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 25.22ba/s]"
     ]
    }
   ],
   "source": [
    "tokenized_datasets_ua = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a682e49e-ad1e-499c-91d0-ee2ecde2697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"roberta-base\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57f8f0b5-54b4-4409-bb14-e0f1ddca2bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrey\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {'params': list(model.roberta.parameters()), 'lr':  0.0000016},\n",
    "    {'params': list(model.classifier.parameters()), 'lr':  0.00012}\n",
    "])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0.1*3*(tokenized_datasets_ua['train'].num_rows/8),\n",
    "    num_training_steps=3*(tokenized_datasets_ua['train'].num_rows/8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "892e129c-4a1a-43a8-936f-cd666eb8570e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ownloading builder script: 100%|█████████████████████████████████████████████████| 6.34k/6.34k [00:00<00:00, 6.33MB/s]"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "label_names = list(label2id.keys())\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff1709fb-86a4-40cb-933b-8cb0f47de171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: krasavjikov. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Andrey\\Documents\\GitHub\\Mountain-NER\\wandb\\run-20241218_140651-u8j5y1p1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/krasavjikov/huggingface/runs/u8j5y1p1' target=\"_blank\">rural-snowball-5</a></strong> to <a href='https://wandb.ai/krasavjikov/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/krasavjikov/huggingface' target=\"_blank\">https://wandb.ai/krasavjikov/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/krasavjikov/huggingface/runs/u8j5y1p1' target=\"_blank\">https://wandb.ai/krasavjikov/huggingface/runs/u8j5y1p1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='990' max='990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [990/990 47:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.044755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.022717</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.719603</td>\n",
       "      <td>0.615058</td>\n",
       "      <td>0.992993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.160500</td>\n",
       "      <td>0.019343</td>\n",
       "      <td>0.658252</td>\n",
       "      <td>0.841191</td>\n",
       "      <td>0.738562</td>\n",
       "      <td>0.995092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.160500</td>\n",
       "      <td>0.019343</td>\n",
       "      <td>0.658252</td>\n",
       "      <td>0.841191</td>\n",
       "      <td>0.738562</td>\n",
       "      <td>0.995092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.160500</td>\n",
       "      <td>0.019343</td>\n",
       "      <td>0.658252</td>\n",
       "      <td>0.841191</td>\n",
       "      <td>0.738562</td>\n",
       "      <td>0.995092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrey\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=990, training_loss=0.09212958017985026, metrics={'train_runtime': 2867.7861, 'train_samples_per_second': 2.762, 'train_steps_per_second': 0.345, 'total_flos': 141244422271920.0, 'train_loss': 0.09212958017985026, 'epoch': 5.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets_ua[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_ua[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bd7bb38-7834-4704-8edf-b67c2fefcfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"D:/models/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fe7e8ca-8b6f-4f8d-930b-db9e355fb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"D:/models/roberta-base\"\n",
    "token_classifier = pipeline(\n",
    "     \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62e36a16-eaf2-49b8-97fa-0485054c1505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': 0.5878781,\n",
       "  'word': 'Alps',\n",
       "  'start': 0,\n",
       "  'end': 4}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier(\"Alps are one of the most beautiful places in Europe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5b561-0081-4c09-9d2a-17ac1f065c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
